{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imports import *\n",
    "from utils.data_loader import download_data, load_data\n",
    "from utils.utils import preprocess_images, get_current_time, calculate_pca, apply_pca_to_rois, GaborPyramid\n",
    "from utils.config import batch_size, num_epochs, model_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [\"../kay_labels.npy\", \"../kay_labels_val.npy\", \"../kay_images.npz\"]\n",
    "urls = [\"https://osf.io/r638s/download\",\n",
    "        \"https://osf.io/yqb3e/download\",\n",
    "        \"https://osf.io/ymnjv/download\"]\n",
    "\n",
    "if download_data(fnames, urls):\n",
    "    init_training_inputs, init_test_inputs, training_outputs, test_outputs, roi, roi_names, labels, val_labels = load_data('../kay_images.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabor Wavelet Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_wavelets: \n",
      "64\n",
      "GaborWaveletPyramid(\n",
      "  (fc): Linear(in_features=64, out_features=8428, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120, 128, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GaborWaveletPyramid(nn.Module):\n",
    "    def __init__(self, max_cycles_per_fov, num_orientations, num_phases, image_resolution, output_size=8428):\n",
    "        super(GaborWaveletPyramid, self).__init__()\n",
    "       \n",
    "        \n",
    "        self.max_cycles_per_fov = max_cycles_per_fov\n",
    "        self.num_orientations = num_orientations\n",
    "        self.num_phases = num_phases\n",
    "        self.image_resolution = image_resolution\n",
    "        self.spatial_frequencies = 2 ** np.arange(np.log2(max_cycles_per_fov))\n",
    "        \n",
    "        # Define parameters of the model\n",
    "        self.num_wavelets = len(self.spatial_frequencies) * num_orientations * num_phases\n",
    "        self.kernel = Parameter(torch.zeros(self.num_wavelets))  # The weights for each wavelet\n",
    "        self.dc_offset = Parameter(torch.zeros(1))  # The DC offset\n",
    "\n",
    "        # Generate Gabor wavelets\n",
    "        self.gabor_wavelets = self.generate_gabor_wavelets(max_cycles_per_fov, num_orientations, num_phases, image_resolution)\n",
    "\n",
    "        #visualize_gabor_wavelets(self.gabor_wavelets)\n",
    "\n",
    "        self.fc = nn.Linear(self.num_wavelets, output_size)\n",
    "        self.precomputed = None\n",
    "\n",
    "    \n",
    "    def generate_gabor_wavelets(self, max_cycles_per_fov, num_orientations, num_phases, image_resolution):\n",
    "\n",
    "        # Initialize the Gabor wavelet tensor\n",
    "        spatial_frequencies = 2 ** np.arange(np.log2(max_cycles_per_fov))\n",
    "        num_wavelets = len(spatial_frequencies) * num_orientations * num_phases\n",
    "        print(\"num_wavelets: \")\n",
    "        print(num_wavelets)\n",
    "        gabor_wavelets = torch.zeros(num_wavelets, image_resolution, image_resolution)\n",
    "        \n",
    "        # Gabor wavelet parameters\n",
    "        sigma = lambda f: 0.56 / f\n",
    "        wavelet_size = lambda f: np.ceil(sigma(f) * 2.5) * 2 + 1  # To ensure an odd-sized filter\n",
    "        \n",
    "        # Generate the wavelets\n",
    "        count = 0\n",
    "        for freq_idx, freq in enumerate(spatial_frequencies):\n",
    "            for orientation in np.linspace(0, np.pi, num_orientations, endpoint=False):\n",
    "                for phase in np.linspace(0, 2*np.pi, num_phases, endpoint=False):\n",
    "                    # Calculate the wavelet size\n",
    "                    filter_sigma = sigma(freq)\n",
    "                    size = np.ceil(filter_sigma * 2.5) * 2 + 1\n",
    "                    size = int(size) if int(size) % 2 == 1 else int(size) + 1  # Ensure the size is odd\n",
    "\n",
    "                    # Generate the Gabor wavelet\n",
    "                    x, y = np.meshgrid(\n",
    "                        np.linspace(-size//2, size//2, size),\n",
    "                        np.linspace(-size//2, size//2, size)\n",
    "                    )\n",
    "                    rotx = x * np.cos(orientation) + y * np.sin(orientation)\n",
    "                    roty = -x * np.sin(orientation) + y * np.cos(orientation)\n",
    "                    \n",
    "                    gabor = np.exp(-(rotx**2 + roty**2) / (2 * sigma(freq)**2)) * np.cos(2 * np.pi * freq * rotx / image_resolution + phase)\n",
    "                    \n",
    "                    # Normalize to zero mean and unit variance\n",
    "                    gabor -= gabor.mean()\n",
    "                    gabor /= gabor.std()\n",
    "\n",
    "                    # Calculate the indices for placement\n",
    "                    half_size = size // 2\n",
    "                    center = image_resolution // 2\n",
    "                    wavelet_slice = slice(center - half_size, center + half_size + 1)\n",
    "                \n",
    "                    # Place the generated wavelet in the tensor\n",
    "                    gabor_wavelets[count, wavelet_slice, wavelet_slice] = torch.from_numpy(gabor)\n",
    "                    count += 1\n",
    "                    \n",
    "        return gabor_wavelets\n",
    "    def compute_gabor_responses(self, images):\n",
    "        \"\"\"Precompute Gabor responses for a batch of images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            batch_responses = []\n",
    "            n = 0\n",
    "            for image in images:\n",
    "                n+=1\n",
    "                #print \"n / totalimages loaded\"\n",
    "                print(n, \"/\", len(images))\n",
    "                image = image.unsqueeze(0) if image.dim() == 2 else image\n",
    "                contrast_energy = torch.zeros(image.size(0), self.num_wavelets, device=image.device)\n",
    "                for i, wavelet in enumerate(self.gabor_wavelets):\n",
    "                    response = nn.functional.conv2d(image, wavelet.unsqueeze(0).unsqueeze(0), padding='same')\n",
    "                    energy = response**2\n",
    "                    contrast_energy[:, i] = energy.view(energy.size(0), -1).sum(dim=1)\n",
    "                batch_responses.append(contrast_energy.sqrt())\n",
    "            return torch.cat(batch_responses)\n",
    "        \n",
    "    def compute_and_save_gabor_responses(self, images, file_path):\n",
    "        \"\"\"Precompute Gabor responses for a batch of images and save to a file.\"\"\"\n",
    "        if isinstance(images, np.ndarray):\n",
    "            images = torch.from_numpy(images)\n",
    "        responses = self.compute_gabor_responses(images)\n",
    "        torch.save(responses, file_path)\n",
    "        print(f\"Saved Gabor responses to {file_path}\")\n",
    "\n",
    "    def load_precomputed_gabor_responses(self, file_path):\n",
    "        \"\"\"Load precomputed Gabor responses from a file.\"\"\"\n",
    "        self.precomputed = torch.load(file_path)\n",
    "        print(f\"Loaded Gabor responses from {file_path}\")\n",
    "        \n",
    "    def forward(self, image):\n",
    "\n",
    "        contrast_energy = self.precomputed if self.precomputed is not None else self.compute_gabor_responses(image)\n",
    "\n",
    "        output = self.fc(contrast_energy)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example instantiation of the model\n",
    "# These values should be determined based on the specifics of the experiment and data\n",
    "max_cycles_per_fov = 16\n",
    "num_orientations = 8\n",
    "num_phases = 2\n",
    "image_resolution = 64  # Example resolution, actual value should be based on the data\n",
    "\n",
    "# Create the Gabor Wavelet Pyramid model\n",
    "gwp_model = GaborWaveletPyramid(max_cycles_per_fov, num_orientations, num_phases, image_resolution)\n",
    "\n",
    "print(gwp_model)\n",
    "\n",
    "#gwp_model.forward(init_test_inputs[0])\n",
    "\n",
    "init_test_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = torch.from_numpy(inputs).float().unsqueeze(1)  # Add channel dimension\n",
    "        self.outputs = torch.from_numpy(outputs).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "#first 2 training inputs\n",
    "train_inputs2 = init_training_inputs[:10]\n",
    "train_outputs2 = training_outputs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define your dataset (assuming MyDataset is your custom dataset class)\n",
    "full_dataset = MyDataset(init_training_inputs, training_outputs)\n",
    "\n",
    "# Split your dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define a simple training loop with early stopping\n",
    "def train_model_with_early_stopping(model, train_loader, val_loader, epochs, learning_rate, patience):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_no_improve = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        print(\"In epoch\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(\"in train loop\")\n",
    "        for inputs, targets in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            print(\"in val loop\")\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_model = deepcopy(model.state_dict())  # Save the best model\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(best_model, f'../trained_models/gabor/val_model_bests_of_all.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Plot loss curve\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.title(f'{model_str} - Encoder Loss')\n",
    "            plt.plot(train_losses, label=\"Encoder Loss\")\n",
    "            plt.plot(val_losses, label=\"Encoder Validation Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Epoch\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "    plot_losses(train_losses, val_losses)\n",
    "    return model\n",
    "\n",
    "# Initialize your model\n",
    "gwp_model = GaborWaveletPyramid(max_cycles_per_fov=16, num_orientations=8, num_phases=2, image_resolution=128)\n",
    "\n",
    "file_path = 'all_training_inputs.pt'\n",
    "\n",
    "#gwp_model.compute_and_save_gabor_responses(init_training_inputs, file_path)\n",
    "\n",
    "gwp_model.load_precomputed_gabor_responses(file_path)\n",
    "\n",
    "# Train your model with early stopping\n",
    "trained_model = train_model_with_early_stopping(\n",
    "    gwp_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=5000,  # Number of epochs to train for\n",
    "    learning_rate= 0.001, #0.001,\n",
    "    patience=10  # Number of epochs to wait for improvement before stopping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try model predicting data from a dataloader\n",
    "def predicted_actual_values(model, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    all_preds = []\n",
    "    all_actual = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            # Ensure data is on the same device as the model\n",
    "            data = data.to(device, dtype=torch.float)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            #outputs, _ = model.forward_with_intermediate(data)\n",
    "            outputs = model.forward(data)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_actual.extend(targets.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_actual)\n",
    "\n",
    "# Compute Root Mean Squared Error\n",
    "def compute_rmse(predictions, actual):\n",
    "    rmse = mean_squared_error(actual, predictions, squared=False)\n",
    "    return rmse\n",
    "\n",
    "# Compute R-squared (Coefficient of Determination)\n",
    "def compute_r2_score(predictions, actual):\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    return r2\n",
    "\n",
    "# Compute Pearson Correlation Coefficient\n",
    "def compute_pearson_correlation(predictions, actual):\n",
    "    # Flatten the predictions and actual arrays in case they have more than one dimension\n",
    "    predictions_flat = predictions.flatten()\n",
    "    actual_flat = actual.flatten()\n",
    "    correlation, _ = pearsonr(predictions_flat, actual_flat)\n",
    "    return correlation\n",
    "\n",
    "# Try running trained model on test data\n",
    "def test_trained_model(model, batch_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #x_test = reshaped_gabor\n",
    "    x_test = init_test_inputs\n",
    "    y_test = test_outputs\n",
    "    #y_test = test_outputs\n",
    "\n",
    "    # Create tensor from training inputs and targets\n",
    "    x_test_tensor, y_test_tensor = torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float()\n",
    "    test_data_tensor = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    # Create dataloader from tensor\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data_tensor, batch_size, shuffle=True)\n",
    "    \n",
    "    # Compute accuracy of model predictions\n",
    "    predicted_results, actual_results = predicted_actual_values(model, test_dataloader)\n",
    "    print('Predictions:')\n",
    "    print(predicted_results[0][0:10])\n",
    "    print('Actual:')\n",
    "    print(actual_results[0][0:10])\n",
    "\n",
    "    rmse = compute_rmse(predicted_results, actual_results)\n",
    "    r2 = compute_r2_score(predicted_results, actual_results)\n",
    "    pearson_correlation = compute_pearson_correlation(predicted_results, actual_results)\n",
    "    \n",
    "    print(f'Achieved RMSE: {rmse:.2f}')\n",
    "    print(f'R-squared: {r2:.2f}')\n",
    "    print(f'Pearson Correlation Coefficient: {pearson_correlation:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_wavelets: \n",
      "64\n",
      "Loaded Gabor responses from test_inputs_gabor.pt\n",
      "Model loaded: AlexNet\n",
      "Predictions:\n",
      "[-0.17746465 -0.08122709  0.08913706 -0.12076683 -0.01345443 -0.0484757\n",
      " -0.04273073  0.06267595  0.06798709 -0.04870605]\n",
      "Actual:\n",
      "[-0.770339   -0.29902986  0.15490855 -0.08567116 -0.12409987  0.2816567\n",
      "  0.0260905  -0.6763842   0.17463025  0.5590392 ]\n",
      "Achieved RMSE: 0.49\n",
      "R-squared: -0.13\n",
      "Pearson Correlation Coefficient: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "#model = trained_model\n",
    "\n",
    "model = GaborWaveletPyramid(max_cycles_per_fov=16, num_orientations=8, num_phases=2, image_resolution=128)\n",
    "\n",
    "#model.compute_and_save_gabor_responses(init_training_inputs[:1], \"test_inputs_gabor1.pt\")\n",
    "model.load_precomputed_gabor_responses(\"test_inputs_gabor.pt\")\n",
    "\n",
    "print('Model loaded:', model_str) \n",
    "model.load_state_dict(torch.load('../trained_models/gabor/val_model_bestss.pth'))\n",
    "\n",
    "\n",
    "\n",
    "# Test model\n",
    "test_trained_model(model=model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES FROM EXPERMENTING\n",
    "\n",
    "3 droputs of 0.9 seemed to be promising but stagnated\n",
    "\n",
    "3 dropouts of 0.85 stagnated after crossing \n",
    "\n",
    "3 dopouts of 0.75\n",
    "\n",
    "\n",
    "Next time, try between 0.75 and 0.85\n",
    "otherwise between 0.85 and 0.9\n",
    "\n",
    "Implement feature layer output saved as pickle file to run model faster\n",
    "\n",
    "Try with another dateset to confirm wehter it is just a \"lorte dataset\"\n",
    "\n",
    "Save every 10 epoch as well as the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**\n",
    "\n",
    "1. **RMSE**: Root Mean Squared Error is the square root of the mean of the squared errors. The RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction. A lower RMSE is better as it indicates a closer fit of the model to the data.\n",
    "2. **R2**: R-squared values typically range from 0 to 1 and can be interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variables. Negative values of R-squared indicate that the model fits the data worse than a horizontal hyperplane at the mean of the dependent variable. This suggests that the model is not capturing the variance of the data well and is performing poorly on this task. It could be due to an overfitted model, a wrong model choice, or irrelevant features.\n",
    "3. **Pearson correlation**: Pearson correlation coefficient measures the linear correlation between two variables. The coefficient values range between -1 and 1. A value close to 1 implies that there is a strong positive correlation between the two variables. A value close to -1 implies that there is a strong negative correlation between the two variables. A value close to 0 implies that there is no linear correlation between the two variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize intermediate outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADMAL-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
